{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 05: Audio Retrieval\n",
    "\n",
    "Query the CLAP embedding index using audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bg-section",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "CLAP (Contrastive Language-Audio Pretraining) maps audio into a 512-dimensional embedding space. Similar sounds cluster together, enabling **content-based retrieval**: given an audio query, find the most similar tracks in the database.\n",
    "\n",
    "Audio queries require preprocessing to match the index:\n",
    "- Center by database mean (removes dataset bias)\n",
    "- L2-normalize (enables cosine similarity via dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "from search.query import embed_audio, preprocess_query, retrieve_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"laion/larger_clap_music\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ClapModel.from_pretrained(MODEL_ID).to(device).eval()\n",
    "processor = ClapProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "index-header",
   "metadata": {},
   "source": [
    "## Load Index\n",
    "\n",
    "The index contains:\n",
    "- `track_ids`: song identifiers\n",
    "- `embeddings`: centered + L2-normalized vectors (n, 512)\n",
    "- `mean`: database mean used for centering (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = project_root / \"notebooks/data/embeddings/clap_index.npz\"\n",
    "index_data = np.load(index_path, allow_pickle=False)\n",
    "\n",
    "track_ids = index_data[\"track_ids\"]\n",
    "embeddings = index_data[\"embeddings\"]\n",
    "mean = index_data[\"mean\"]\n",
    "\n",
    "print(f\"Index: {len(track_ids)} tracks, {embeddings.shape[1]}-d embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(project_root / \"notebooks/data/merge_preprocessed.csv\")\n",
    "track_to_meta = {str(row[\"song_id\"]): row for _, row in df_meta.iterrows()}\n",
    "print(f\"Metadata: {len(df_meta)} tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-header",
   "metadata": {},
   "source": [
    "## Retrieval Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "self-retrieval-header",
   "metadata": {},
   "source": [
    "### Self-Retrieval Test\n",
    "\n",
    "Sanity check: querying with an existing track's embedding should return itself as top-1 with score 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 0\n",
    "q_self = embeddings[test_idx]\n",
    "results_self = retrieve_top_k(q_self, embeddings, track_ids, k=5)\n",
    "\n",
    "print(f\"Query: {track_ids[test_idx]}\")\n",
    "print(f\"Top-1: {results_self[0][0]} (score: {results_self[0][1]:.4f})\")\n",
    "assert results_self[0][0] == str(track_ids[test_idx]), \"Self-retrieval failed!\"\n",
    "assert abs(results_self[0][1] - 1.0) < 1e-6, \"Score should be 1.0\"\n",
    "print(\"\\nSelf-retrieval test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audio-query-header",
   "metadata": {},
   "source": [
    "### Audio Query\n",
    "\n",
    "Query with an audio file. The file is embedded, centered using the database mean, and L2-normalized before retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "audio-query-embed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_audio = project_root / \"notebooks/data/audio/sample-3.mp3\"\n",
    "\n",
    "if query_audio.exists():\n",
    "    q_raw = embed_audio(query_audio, model, processor, device)\n",
    "    q = preprocess_query(q_raw, mean)\n",
    "    print(f\"Query: {query_audio.name}\")\n",
    "    print(f\"Raw norm: {np.linalg.norm(q_raw):.4f}, Preprocessed norm: {np.linalg.norm(q):.4f}\")\n",
    "else:\n",
    "    print(f\"Query audio not found: {query_audio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "audio-query-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_audio.exists():\n",
    "    print(\"Query audio:\")\n",
    "    display(Audio(query_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5r089rvozlu",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_audio.exists():\n",
    "    results = retrieve_top_k(q, embeddings, track_ids, k=10)\n",
    "    print(\"Top-10 results:\\n\")\n",
    "    for rank, (tid, score) in enumerate(results, 1):\n",
    "        meta = track_to_meta.get(tid, {})\n",
    "        artist = meta.get(\"artist\", \"?\") if isinstance(meta, dict) else getattr(meta, \"artist\", \"?\")\n",
    "        title = meta.get(\"title\", \"?\") if isinstance(meta, dict) else getattr(meta, \"title\", \"?\")\n",
    "        audio_path = meta.get(\"audio_path\", None) if isinstance(meta, dict) else getattr(meta, \"audio_path\", None)\n",
    "        print(f\"{rank:2}. {artist} - {title} ({score:.4f})\")\n",
    "        if audio_path and Path(audio_path).exists():\n",
    "            display(Audio(audio_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates audio-to-audio retrieval using the CLAP embedding index. Query audio is embedded, centered by the database mean, and L2-normalized before computing cosine similarity against indexed tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Index: {len(track_ids)} tracks, {embeddings.shape[1]}-d\")\n",
    "print(f\"Model: {MODEL_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
