{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d756f9",
   "metadata": {},
   "source": [
    "# 03: Feature Extraction\n",
    "\n",
    "Extract audio features and CLAP embeddings from the preprocessed dataset.\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook generates two complementary representations of audio content:\n",
    "\n",
    "1. **Audio features** (rhythm, spectral, tonal) capture low-level acoustic properties derived from signal processing. These 56 features describe tempo, timbre, harmonic content, and rhythmic patterns.\n",
    "\n",
    "2. **CLAP embeddings** (512-d) capture high-level semantic audio content through a contrastive language-audio model. These embeddings encode musical similarity in a learned latent space.\n",
    "\n",
    "Both representations are used downstream for manifold learning and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1d013",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import asdict\n",
    "\n",
    "import essentia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# suppress essentia HPCP normalization warning (informational, not an error)\n",
    "essentia.log.warningActive = False\n",
    "\n",
    "from configs.dataset import load_config\n",
    "from core.embed import CLAP_BATCH_SIZE, center_and_normalize, extract, save_embeddings\n",
    "from eval.embed import check_embedding_sanity\n",
    "from features.parallel import extract_features_parallel\n",
    "from models.clap import ClapEmbedder\n",
    "\n",
    "N_WORKERS = os.cpu_count() or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecf6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(project_root / \"configs\" / \"config.yaml\")\n",
    "print(f\"Dataset root: {config.paths.dataset_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(project_root / \"notebooks/data/merge_preprocessed.csv\")\n",
    "print(f\"Loaded {len(df)} tracks\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ea0a5",
   "metadata": {},
   "source": [
    "## Feature Extraction (Audio Features)\n",
    "\n",
    "Extract rhythm, spectral, and tonal features from each audio file using librosa and essentia.\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "| Category | Count | Description |\n",
    "|----------|-------|-------------|\n",
    "| Rhythm | 23 | Tempo, onset strength, beat intervals, BPM histogram, rhythm transform, beats loudness |\n",
    "| Spectral | 10 | MFCC 1-5, spectral centroid, contrast, inharmonicity |\n",
    "| Tonal | 23 | Chroma entropy, scale alignment, HPCP 0-11, key strength, key encoding |\n",
    "\n",
    "### Feature Field Reference\n",
    "\n",
    "| Category | Field | Description |\n",
    "|----------|-------|-------------|\n",
    "| Rhythm | `tempo_bpm` | Estimated tempo in beats per minute |\n",
    "| Rhythm | `beat_interval_cv` | Coefficient of variation of beat intervals |\n",
    "| Rhythm | `bpm_histogram_entropy` | Shannon entropy of BPM distribution |\n",
    "| Spectral | `mfcc_1_mean` to `mfcc_5_mean` | Mel-frequency cepstral coefficients |\n",
    "| Spectral | `spectral_centroid_mean` | Center of mass of spectrum |\n",
    "| Spectral | `inharmonicity_mean` | Deviation from harmonic series |\n",
    "| Tonal | `chroma_entropy` | Entropy of 12-bin chromagram |\n",
    "| Tonal | `key_strength` | Confidence of detected key (0-1) |\n",
    "| Tonal | `hpcp_0` to `hpcp_11` | Harmonic pitch class profile |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = df[\"audio_path\"].tolist()\n",
    "song_ids = df[\"song_id\"].tolist()\n",
    "\n",
    "print(f\"Extracting features from {len(audio_paths)} tracks using {N_WORKERS} workers\")\n",
    "\n",
    "features_list = extract_features_parallel(audio_paths, song_ids, n_workers=N_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(features_list)\n",
    "print(f\"Shape: {features_df.shape} (expected: {len(df)} rows, 57 columns)\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f211197",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = project_root / \"notebooks/data/merge_audio_features.csv\"\n",
    "features_df.to_csv(features_path, index=False)\n",
    "print(f\"Saved audio features to {features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646cf074",
   "metadata": {},
   "source": [
    "## CLAP Embedding Extraction\n",
    "\n",
    "Extract 512-dimensional CLAP embeddings using the `laion/larger_clap_music` model. These embeddings capture high-level semantic audio content through contrastive learning between audio and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5359079",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ClapEmbedder.from_pretrained(\"laion/larger_clap_music\")\n",
    "print(f\"Device: {embedder.device}\")\n",
    "print(f\"Sample rate: {embedder.sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ebea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = [Path(p) for p in df[\"audio_path\"]]\n",
    "track_ids = df[\"song_id\"].tolist()\n",
    "\n",
    "print(f\"Extracting embeddings for {len(audio_paths)} tracks\")\n",
    "print(f\"Batch size: {CLAP_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefee3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "n_batches = (len(audio_paths) + CLAP_BATCH_SIZE - 1) // CLAP_BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"Extracting CLAP embeddings\"):\n",
    "    start = i * CLAP_BATCH_SIZE\n",
    "    end = min(start + CLAP_BATCH_SIZE, len(audio_paths))\n",
    "\n",
    "    batch_paths = audio_paths[start:end]\n",
    "    batch_ids = track_ids[start:end]\n",
    "\n",
    "    batch_embeddings = extract(batch_paths, embedder, track_ids=batch_ids, batch_size=CLAP_BATCH_SIZE)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "print(f\"Extracted {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78876204",
   "metadata": {},
   "source": [
    "### Embedding Validation\n",
    "\n",
    "Check for common pathologies: NaN values, infinite values, zero-norm vectors, and embedding spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.vstack([e.embedding for e in embeddings])\n",
    "sanity = check_embedding_sanity(emb_matrix)\n",
    "\n",
    "print(f\"Shape: {sanity.n_samples} x {sanity.n_dims}\")\n",
    "print(f\"NaN: {sanity.has_nan}, Inf: {sanity.has_inf}, Zero-norm: {sanity.has_zero_norm}\")\n",
    "print(f\"Mean pairwise cosine: {sanity.mean_pairwise_cosine:.3f} +/- {sanity.std_pairwise_cosine:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8ilz24t5g",
   "metadata": {},
   "source": [
    "### Centering and Normalization\n",
    "\n",
    "The high mean pairwise cosine (0.94) indicates embeddings cluster in a narrow cone, a common pathology in contrastive models. Centering at the origin then re-normalizing breaks this geometry, improving isotropy and making cosine similarity more discriminative for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lwiwvd8jp1p",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_normalized = center_and_normalize(embeddings)\n",
    "print(f\"Centered and normalized {len(embeddings_normalized)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1aigc88iwa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix_normalized = np.vstack([e.embedding for e in embeddings_normalized])\n",
    "sanity_normalized = check_embedding_sanity(emb_matrix_normalized)\n",
    "\n",
    "print(f\"Shape: {sanity_normalized.n_samples} x {sanity_normalized.n_dims}\")\n",
    "print(f\"NaN: {sanity_normalized.has_nan}, Inf: {sanity_normalized.has_inf}, Zero-norm: {sanity_normalized.has_zero_norm}\")\n",
    "print(f\"Mean pairwise cosine: {sanity_normalized.mean_pairwise_cosine:.3f} +/- {sanity_normalized.std_pairwise_cosine:.3f}\")\n",
    "print(f\"Reduction: {sanity.mean_pairwise_cosine:.3f} -> {sanity_normalized.mean_pairwise_cosine:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41950a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = project_root / \"notebooks/data/embeddings\"\n",
    "embeddings_dir.mkdir(exist_ok=True)\n",
    "\n",
    "embeddings_path = embeddings_dir / \"clap_embeddings.npz\"\n",
    "save_embeddings(embeddings, embeddings_path)\n",
    "print(f\"Saved embeddings to {embeddings_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reyf942lst",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_normalized_path = embeddings_dir / \"clap_embeddings_normalized.npz\"\n",
    "save_embeddings(embeddings_normalized, embeddings_normalized_path)\n",
    "print(f\"Saved normalized embeddings to {embeddings_normalized_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6r3a7rnfzx9",
   "metadata": {},
   "source": [
    "### Retrieval Index\n",
    "\n",
    "Create a retrieval index that includes the database mean vector. Queries must be centered using this mean (not their own mean) to ensure consistent preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3x5nfm3n9g4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retrieval index with saved mean\n",
    "emb_matrix_raw = np.vstack([e.embedding for e in embeddings])\n",
    "mean_vector = emb_matrix_raw.mean(axis=0)\n",
    "centered = emb_matrix_raw - mean_vector\n",
    "norms = np.linalg.norm(centered, axis=1, keepdims=True)\n",
    "emb_matrix_norm = centered / norms\n",
    "\n",
    "index_path = embeddings_dir / \"clap_index.npz\"\n",
    "np.savez_compressed(\n",
    "    index_path,\n",
    "    track_ids=np.array([e.track_id for e in embeddings]),\n",
    "    embeddings=emb_matrix_norm.astype(np.float32),\n",
    "    mean=mean_vector.astype(np.float32),\n",
    ")\n",
    "print(f\"Saved retrieval index to {index_path}\")\n",
    "print(f\"  track_ids: {len(embeddings)}\")\n",
    "print(f\"  embeddings: {emb_matrix_norm.shape}\")\n",
    "print(f\"  mean: {mean_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08be3b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Outputs:**\n",
    "- `notebooks/data/merge_audio_features.csv`: Audio features (song_id + 56 features)\n",
    "- `notebooks/data/embeddings/clap_embeddings.npz`: Raw CLAP embeddings (track_ids + 512-d vectors)\n",
    "- `notebooks/data/embeddings/clap_embeddings_normalized.npz`: Centered + normalized embeddings for improved isotropy\n",
    "- `notebooks/data/embeddings/clap_index.npz`: Retrieval index (track_ids + embeddings + mean vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature extraction complete.\")\n",
    "print(f\"  Audio features: {features_df.shape[0]} tracks, {features_df.shape[1] - 1} features\")\n",
    "print(f\"  CLAP embeddings: {len(embeddings)} tracks, {embeddings[0].dim} dimensions\")\n",
    "print(f\"  Normalized embeddings: {len(embeddings_normalized)} tracks (mean cosine: {sanity_normalized.mean_pairwise_cosine:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
